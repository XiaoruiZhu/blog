---
layout: post
title: Simulation of Evalution Matrics in Machine Learning
categories: blog, Statistics, R, Machine Learning
excerpt: Simulation of Evalution Matrics in Machine Learning
tags: [R, Statistics, Machine Learning]
comments: true
share: true
image:
feature:
date: Dec 23, 2016
---

This post is the simulation work for many different evaluation matrics in Machine Learning filed. In order to show what matrics work in certain situations, for different setting (including balanced data, imbalanced data), this study shows performance of different matrics so that the applicabilities of each one can be seen clearly. 

# Evaluation Matrics

In machine learning filed, various different performance evaluation matrics are available for the selection of models and algorithms. Having a lot of tools in your hand never means a bad news. The potential problems, however, include which one is the best for my situation, why use this one, is this one robust for outliers or imbalance. Even the matrics is good enough, does the one you choicen have good performance when the data size increase? These question inspire me to do these simulations in this post. 

On the other hand, this post will be helpful for someone who want to learn and implement these matrics in his or her projects. Simultaneously, better understaning leads better choices and decisions. So let's start. 

## AUC-ROC (Area under receiver operating characteristic curve)

It's easy and may be one of the most popular matric among academics. In pratical, it seems not that popular. It will show the performance of the binary classifier by changing threshold probability. 

## AUC-PR (Area under precision recall curve)

## Hosmer-Lemeshow test

## Decile table

## 
